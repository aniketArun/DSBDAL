# -*- coding: utf-8 -*-
"""1datawrangling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1unzFyTmPuMj2YbSF8RA9H7cf-GOwqaV3

<h1>Assignment no 1</h1>
<pre>1. Locate open Iris dataset from the URL csv_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'
2. Perform the following operation on dataset
a) Display total no of rows and column
b) Display type of each column
c) Sort the data in descending order , by considering column sepal.length
d) Slice the data: 11 to 20 rows, and only two columns, sepal.length and Species
e) rename the column Species to Type</pre>
"""

#import all neccessary modules
import pandas as pd

#import the dataset from url
csv_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'
columns = ['sepal.length', 'sepal.width', 'petal.length', 'petal.width', 'Species']
df = pd.read_csv(csv_url, header = None, names = columns)

df

df.columns

#a => no of rows and columns
rows, cols = df.shape

print("Rows = ", rows, "\tColumns = ", cols)

#b => display the type of each cols

df.dtypes

#c => sort the values using column sepal.length
df.sort_values(by = 'sepal.length', ascending = False)

#d => display the datat only of the 10 to 20
df.loc[10:19, ['sepal.length', 'Species']]

#e => rename the species column to type
df.rename({'Species':'Type'}, inplace = True)
df

# -*- coding: utf-8 -*-
"""2DataWraggling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1b5OKkzoLke82LbwSd_9XDFHi3AD1Il8r
"""

import pandas as pd
import numpy as np

df=pd.read_csv("student_dataset.csv") #read the dataset
df

#plot the graph using boxplot for each column
col = ['math score', 'reading score' , 'writing score','placement score']
df.boxplot(col)

print(np.where(df['math score']>90))
print(np.where(df['reading score']<25))
print(np.where(df['writing score']<30))

"""Import Matplotlib for graph plotting"""

import matplotlib.pyplot as plt

"""Detecting the outlires using the scatter plot agains placement score and placement offer"""

fig, ax = plt.subplots(figsize = (18,10))
ax.scatter(df['placement score'], df['placement offer count'])
plt.show()

"""Print the outlies with reference the scatter plot, **It will going to return a tuple**"""

print(np.where((df['placement score']<50) & (df['placement offer count']>1)))
print(np.where((df['placement score']>85) & (df['placement offer count']<3)))

"""Print the outlies with values using iloc function over dataframe"""

df.iloc[6]

df.iloc[11]

"""**Detecting the outliers using the z-score **"""

# import the scipy lib
from scipy import stats

"""Calculate the z score for the math column"""

z = np.abs(stats.zscore(df['math score']))
z

"""Define the threshold for the zscore"""

threshold = 0.18

#display the sample outlires
sample_outliers = np.where(z <threshold)
sample_outliers

for i in sample_outliers:
  print(df.iloc[i])

"""**Detecting outliers using Inter Quantile Range(IQR):**

Sort the reading score
"""

sorted_rscore= sorted(df['reading score'])
sorted_rscore

"""Calculate and print Quartile 1 and Quartile 2"""

q1 = np.percentile(sorted_rscore, 25)
q3 = np.percentile(sorted_rscore, 75)
print(q1,q3)

"""Calculate the IQR"""

IQR = q3-q1
IQR

"""Calculate the upper bound and the lower bound"""

lwr_bound = q1-(1.5*IQR)
upr_bound = q3+(1.5*IQR)
print("Lower Bound : ", lwr_bound, "Upper Bound: ", upr_bound)

"""Print the outliers as we have the upper and lower bound"""

r_outliers = [i for i in sorted_rscore if (i < lwr_bound or i > upr_bound)]
# for i in sorted_rscore:
#   if (i < lwr_bound or i  >upr_bound):
#     r_outliers.append(i)
print(r_outliers)

"""**HANDLING THE OUTLIERS**

Trimming/removing the outlier
"""

new_df = df
for i in sample_outliers:
  new_df.drop(i,inplace = True)
new_df #here outliers with index in sample_outliers are deleted

"""**Quantile based flooring and capping**

90th percentile and 10th percentile

percentile : that means you scored better than 90% of people who took the test and have performed well compared to others.
**A percentile provides more granular information regarding the performance of your test, which is obtained by comparing it with other tests in the database.**
"""

df_stud = df
ninetieth_percentile = np.percentile(df_stud['math score'], 90)
b = np.where(df_stud['math score']>ninetieth_percentile, ninetieth_percentile, df_stud['math score'])
print("New array:", b)

df_stud.insert(1,"m score",b,True)
df_stud

"""**Mean/Median imputation**
Plot the box plot for reading score
"""

col = ['reading score']
df.boxplot(col)

"""Calculate the median of reading score by using sorted_rscore"""

median=np.median(sorted_rscore)
median

"""Replace the upper bound outliers using median value"""

refined_df=df
refined_df['reading score'] = np.where(refined_df['reading score'] > upr_bound, median,refined_df['reading score'])

"""Display the refined Dataframe"""

refined_df

"""Replace the lower bound outliers using median value"""

refined_df['reading score'] = np.where(refined_df['reading score'] <lwr_bound, median,refined_df['reading score'])
refined_df

"""Draw the box plot for redefined_df"""

col = ['reading score']
refined_df.boxplot(col)

# -*- coding: utf-8 -*-
"""3LabelEncodingIris.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15WSrkSuAI4loNRKbMEVNk9OG2vnULlRN

<pre>
1. Locate open Iris dataset from the URL csv_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'
2. Perform the label encoding , by considering Species as target variable.
</pre>
"""

import pandas as pd
from sklearn.preprocessing import LabelEncoder

csv_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'
cols = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'Species']
df = pd.read_csv(csv_url, names = cols)

df

df.columns

le = LabelEncoder()

df['Species'] = le.fit_transform(df['Species'])

df['Species']

df.head()

df['Species'].unique()

# -*- coding: utf-8 -*-
"""4irisOneHotEncode.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bJpL-hHhIYUWghc6EenUK2ia1do9vEDr

<pre>
1. Locate open Iris dataset from the URL csv_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'
2. Perform the One Hot encoding , by considering Species as target variable.

</pre>
"""

import pandas as pd
from sklearn.preprocessing import OneHotEncoder

csv_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'
cols = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'Species']
df = pd.read_csv(csv_url, names = cols)

df

df['Species'].unique()

ohe = OneHotEncoder(handle_unknown = 'ignore',sparse_output= False).set_output(transform = 'pandas')
# ohe = OneHotEncoder()
df_transform = ohe.fit_transform(df[['Species']])

df_transform.head()

# concate the df
df = pd.concat([df, df_transform], axis = 1).drop(columns = ['Species'])

df.head()

ohe  = OneHotEncoder(handle_unknown='ignore', sparse_output=False).set_output(transform = 'pandas')

# -*- coding: utf-8 -*-
"""5IrisDummyEncoding.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oybO3D6q5aT86tRieuOYKyrjF9sK9oDr

<pre>
1. Locate open Iris dataset from the URL csv_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'
2. Perform the Dummy Variable Encoding , by considering Species as target variable
</pre>
"""

import pandas as pd

csv_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'
columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']
df = pd.read_csv(csv_url, names = columns)

df.head()

df.tail()

pd.get_dummies(df, columns=['species'])

pd.get_dummies(df['species']).head().astype(int)

pd.get_dummies(df)

# -*- coding: utf-8 -*-
"""7AcademicPerformance.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mk5TfA0OC5N_2RzfADzfDQ3zapVo6GQU

<pre>
1. Load the Academic Performance dataset in data frame object.
2. Check null values in the dataset.
3. Check missing values in dataset and replace the null values with standard null value NaN
4. Replace the missing value of Math Score with Mean Value
5. Replace the missing value of Reading Score with standard deviation
6. Replace the missing value of place with common value "Nashik"
</pre>
"""

import pandas as pd

df = pd.read_csv('AcademicPerformance.csv', na_values=['na', 'Na'])
df

#Check for Null values in dataset
df.isnull()

df.isnull().sum()

df['math score'].mean()

#repalce nan value of Math_Score with mean value
df['math score'].fillna(value = df['math score'].mean(), inplace = True)

df['math score']

#  Replace the missing value of Reading Score with standard deviation
df['reading score'].fillna(value = df['reading score'].std(), inplace = True)

df['reading score']

# Replace the missing value of place with common value "Nashik"
df['Region'].fillna(value = 'Nashik', inplace = True)

df

# -*- coding: utf-8 -*-
"""8AcedemicPerformance.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10G0Ze7It634-gFkgb3kUo6rIouGFnqJ8

<pre>
1. Load the Academic Performance dataset in data frame object.
2. Check null values in the dataset.
3. Count the number of null values in complete data set (Hint: eplace the null values with standard null value NaN)
4. Dropping rows with at least 1 null value
5. Dropping rows if all values in that row are missing
6. Dropping columns with at least 1 null value.
7. Dropping Rows with at least 1 null value in CSV file
</pre>
"""

import pandas as pd

df = pd.read_csv('AcademicPerformance.csv', header= None, na_values=['na','Na'])
df

#Check null values in the dataset.
df.isna().sum()

ndf = df #Copy the dataset

# Dropping rows with at least 1 null value
ndf.dropna()

ndf.dropna(how = 'all')
# Dropping rows if all values in that row are missing

ndf.dropna(axis = 1)
# Dropping columns with at least 1 null value.

# Dropping Rows with at least 1 null value in CSV file
ndf.dropna(axis = 0, how = 'any')



# -*- coding: utf-8 -*-
"""9demoDfBox_Qt.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GIQJ-UsANk9Yar80BksSEw4shqBdFhN6

<pre>
1. Load the demo dataset in dataframe object df
2. Detect the outlier using BoxPlot.
3. Handle the outlier using Quantile based flooring and capping
  (Hint: the outlier is capped at a certain value above the 90th percentile value
or floored at a factor below the 10th percentile value)
</pre>
"""

import pandas as pd
import numpy as np

df = pd.read_csv('demo.csv')
df.head(10)

#plot the box plot
cols = ['math score',
 'reading score',
 'writing score',
 'placement score',
        'placement offer count'
  ]
cols

df.boxplot(cols)

#print the outlier

print(np.where(df['math score'] > 90))

np.where(df['reading score'] < 25)

np.where(df['writing score'] < 25)

np.where(df['placement score'] < 50)

#quantile based flooring and capping
df_stud=df
ninetieth_percentile = np.percentile(df_stud['math score'], 90)
b = np.where(df_stud['math score']>ninetieth_percentile ,ninetieth_percentile, df_stud['math score'])
print("New array:",b)

df_stud.insert(1,"m score",b,True)
df_stud

df['math score'].skew()

df['m score'].skew()

# -*- coding: utf-8 -*-
"""10DemoScatterQtl.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zUBwkNzM4e2cg_2JY12FIYwxAuwZH2XA

<pre>
1. Load the demo dataset in dataframe object df
2. Detect the outlier using ScatterPlot
3. Handle the outlier using Quantile based flooring and capping
  (Hint: the outlier is capped at a certain value above the 90th percentile value
or floored at a factor below the 10th percentile value)
</pre>
"""

import pandas as pd
import numpy as np

df = pd.read_csv('demo.csv')
df

#plot the scatter plot
cols = ['math score',
 'reading score',
 'writing score',
 'placement score',
        'placement offer count'
  ]
cols

import matplotlib.pyplot as plt

plt.scatter(df['placement score'], df['placement offer count'])

df_stud = df
ninetieth_percentile = np.percentile(df_stud['math score'], 90)
b = np.where(df_stud['math score']>ninetieth_percentile, ninetieth_percentile, df_stud['math score'])
print("New array:",b)
df_stud.insert(1,"m score",b,True)
df_stud

# -*- coding: utf-8 -*-
"""11DemoMeanMode.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f-xhhglUw3mVoNHCi6IEdRZDpIGM-ZIG

<pre>
1. Load the demo dataset in dataframe object df
2. Detect the outlier using Z-score
3. replace the outliers with the median value.
</pre>
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

df = pd.read_csv('demo.csv')
df

# detect outlier using z - score
cols = ['math score',
 'reading score',
 'writing score',
 'placement score',
        'placement offer count'
  ]
cols

z = np.abs(stats.zscore(df['placement score']))
z

th = 0.18
th

outliers = np.where(z < th)

df.boxplot('placement score')

zsc = np.abs(stats.zscore(df))
zsc

# ot = (zsc > 3).any(axis = 1)
ot = (zsc > 3)
ot

df[ot] = df.median()

df

ot

# -*- coding: utf-8 -*-
"""12DemoIQR.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/140AFMIP8gMhJ48mPlPjc-GzJSx72tQJm

<pre>
1. Load the demo dataset in dataframe object df
2. Detect the outlier using Inter Quantile Range(IQR)
3. remove the outliers from the dataset.
</pre>
"""

import pandas as pd
import numpy as np

df = pd.read_csv('demo.csv')
df

rscore = sorted(df['reading score'])

df.head()

rscore

q1 = np.percentile(rscore, 25)

q1

q3 = np.percentile(rscore, 75)
q3

IQR = q3 - q1

lower_bound = q3 - 1.5 * IQR
upper_bound = q3 + 1.5 * IQR

lower_bound, upper_bound

outliers = df[(df['reading score'] < lower_bound) | (df['reading score'] > upper_bound)]

outliers

ninetieth_percentile = np.percentile(df['reading score'], 90)
df.loc[df['reading score'] > upper_bound, 'reading score'] = ninetieth_percentile

# Print the DataFrame with replaced outliers
df.head()

outliers = []
for i in rscore:
   if (i < 56 and i > 92):
    outliers.append(i)
outliers

import seaborn as sns
sns.boxplot(df['reading score'])

for i in outliers:
  df.drop(i, inplace= True)
df

sns.boxplot(df['reading score'])

# -*- coding: utf-8 -*-
"""13MallCustomer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z3RR8QXX7SMCm5vCfGrx3oHsYzPGU_Rg

<pre>
1. Load the MallCustomer dataset in dataframe object df
2. Display summary statistics
(mean, median, minimum, maximum, standard deviation) for a dataset for each column
3. Display Measures of Dispersion
( Mean Absolute Deviation, Variance, Standard Deviation, Range, Quartiles, Skewness)
4. if your categorical variable is age groupsa nd quantitative variable is income, then provide summary statistics
(minimum and maximum) of income
grouped by the age groups
</pre>
"""



import pandas as pd
import numpy as np
import seaborn as sns

df = pd.read_csv('Mall_Customers.csv')
df

df.describe()

df.loc[:, 'Age'].median()

df.min()

new_df = df.drop(['Genre'], axis = 1)

new_df

new_df.median()

new_df.mode()

mean = np.mean(new_df)
mean

#mean absolute deviation
mean_abs_dev = np.mean(abs(new_df - mean))
mean_abs_dev

new_df.var()

new_df.std()

np.percentile(new_df, 25)

new_df.skew()

# summary stats
df.groupby(['Genre'])['Age'].mean()

# -*- coding: utf-8 -*-
"""14BostonHousing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YlH_1kUM3rwN0_5yvIPKgofyNis26g3l

<pre>
Create a Linear Regression Model using Python to predict home prices using Boston Housing Dataset.
The objective is to predict the value of prices of the house using the given features
</pre>
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score

data_url = "http://lib.stat.cmu.edu/datasets/boston"
raw_df = pd.read_csv(data_url, sep="\s+", skiprows=22, header=None)
data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])
target = raw_df.values[1::2, 2]

X = data
y = target

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Create a linear regression model
model = LinearRegression()

# Train the model on the training data
model.fit(X_train_scaled, y_train)

# Predict the prices on the testing data
y_pred = model.predict(X_test_scaled)

# Evaluate the model using mean squared error and R-squared
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error (MSE): {mse:.2f}")
print(f"R-squared (R2): {r2:.2f}")

# Display the model coefficients and intercept
print("\nModel Coefficients:")
for i, coef in enumerate(model.coef_):
    print(f"Feature {coef:.2f}")
print(f"Intercept: {model.intercept_:.2f}")

# You can also print the first 10 actual and predicted home prices
print("\nActual vs Predicted Prices (first 10 samples):")
for i in range(10):
    print(f"Actual: {y_test[i]:.2f}, Predicted: {y_pred[i]:.2f}")

data = pd.DataFrame(data)
data

# -*- coding: utf-8 -*-
"""15SocialNetConfusion.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_qJ3kZpudCgDNkyj2vV_1y3ii3RzUB2t

1. Implement logistic regression using Python to perform classification on Social_Network_Ads.csv dataset.
2. Compute Confusion matrix to find TP, FP, TN, FN, Accuracy, Error rate, Precision, Recall on the given dataset..
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

df = pd.read_csv('Social_Network_Ads.csv')
df.head(10)

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()

df['Gender'] = le.fit_transform(df['Gender'])

df.head()

df.corr()

x = df.drop(['Purchased'], axis = 1)
x

y = df['Purchased']
y.head()

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)

model = LogisticRegression()
model.fit(x_train, y_train)

y_pred = model.predict(x_test)
y_pred

from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay

accuracy_score(y_test,y_pred)

precision_score(y_test,y_pred, average='micro')

recall_score(y_test, y_pred)

cm = confusion_matrix(y_test, y_pred)

ConfusionMatrixDisplay(cm, display_labels = y.unique()).plot()



# -*- coding: utf-8 -*-
"""16NaiveByesClassification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Qm50rbCkPTsStiXD5rQ2rILbajcRcqxs

1. Implement Simple Naïve Bayes classification algorithm using Python/R on iris.csv dataset.
2. Compute Confusion matrix to find TP, FP, TN, FN, Accuracy, Error rate, Precision, Recall on the given dataset
"""

import pandas as pd
import numpy as np
from sklearn.datasets import load_iris

csv_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'
col_names = ['Sepal_Length','Sepal_Width','Petal_Length','Petal_Width','Species']
iris = pd.read_csv(csv_url, names = col_names)

iris.head()

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()

iris['Species'] = le.fit_transform(iris['Species'])

iris.head()

iris.corr()

iris.isna().sum()

x = iris.drop(['Species'], axis = 1)
y = iris['Species']

from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import precision_score, recall_score, accuracy_score
from sklearn.model_selection import train_test_split
nv = GaussianNB()
x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2)

nv.fit(x_train, y_train)

y_pred = nv.predict(x_test)
y_pred

accuracy_score(y_test,y_pred)

precision_score(y_test, y_pred, average='micro')

recall_score(y_test, y_pred, average='micro')

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
cm = confusion_matrix(y_test, y_pred)

ConfusionMatrixDisplay(cm).plot()



# -*- coding: utf-8 -*-
"""18TitanicDataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14y6EuWFeGVQNW0K-nS1H6u3_XAI20tyB

<pre>1. Use the inbuilt dataset 'titanic'.
Use the Seaborn library to see if we can find any patterns in the data.
2. Write a code to check how the price of the ticket (column name: 'fare')
for each passenger is distributed by plotting a histogram</pre>
"""

import pandas as pd
import seaborn as sns

df = sns.load_dataset('titanic')

df

df.isna().sum()

sns.histplot(df['fare'], bins = 10, kde= True, fill= True)

# -*- coding: utf-8 -*-
"""19titanicBoxPlot

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p4A7N3dkSPdSCDZ0UuzW_jmRzgobj9LA

1. Use the inbuilt dataset 'titanic' as used in the above problem. Plot a box plot for distribution of age with respect to each gender along with the
information about whether they survived or not. (Column names : 'sex' and 'age')
2. Write observations on the inference from the above statistics.
"""

import pandas as pd
import seaborn as sns

df = sns.load_dataset("titanic")

df.head()

df.isna().sum()

sns.boxplot(x ='sex', y = 'age', hue = 'survived', data = df)



# -*- coding: utf-8 -*-
"""20irisFlowerComp.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RjeAK6PKFFWzCwRfmMBiJv8mqMRfQyg2

Download the Iris flower dataset or any other dataset into a DataFrame. (e.g.,
https://archive.ics.uci.edu/ml/datasets/Iris ). Scan the dataset and give the inference as:
1. List down the features and their types (e.g., numeric, nominal) available in the dataset.
2. Create a histogram for each feature in the dataset to illustrate the feature distributions.
3. Create a box plot for each feature in the dataset.
4. Compare distributions and identify outliers.
"""

import pandas as pd
import seaborn as sn
import matplotlib.pyplot as plt

df = sn.load_dataset('iris')

df.head()

df.dtypes

fig, axs = plt.subplots(2, 2, figsize=(10, 10))
axs = axs.flatten()

for i, feature in enumerate(df.columns[:-1]):
    sn.histplot(data=df, x=feature, kde=True, ax=axs[i])
    axs[i].set_title(f"Histogram of {feature}")

# plt.tighten_layout()
plt.suptitle('Histograms of Features in Iris Dataset')
plt.show()

plt.figure(figsize=(12, 8))
for i, feature in enumerate(df.columns[:-1]):
    plt.subplot(2, 2, i + 1)
    sn.boxplot(data=df, x=feature)
    plt.title(f"Box Plot of {feature}")

plt.tight_layout()
plt.suptitle('Box Plots of Features in Iris Dataset')
plt.show()

for i, feature in enumerate(df.columns[:-1]):
  sn.boxplot(x= feature, data = df)

ASS17[POS tagging]

1. Extract Sample document and apply following document preprocessing methods: Tokenization, POS Tagging, stop words removal, Stemming and
Lemmatization.
2. Create representation of document by calculating Term Frequency and Inverse Document Frequency.


CODE:

POSTagging

import nltk
import re

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

text= "Tokenization is the first step in text analytics. The process of breaking down a text paragraph into smaller chunks such as words or sentences is called Tokenization."


from nltk.tokenize import sent_tokenize

from nltk.corpus import stopwords
stop_words=set(stopwords.words("english"))
print(stop_words)
text= "How to remove stop words with NLTK library in Python?"
text= re.sub('[^a-zA-Z]', ' ',text)
tokens = word_tokenize(text.lower())
filtered_text=[]
for w in tokens:
    if w not in stop_words:
        filtered_text.append(w)
print("Tokenized Sentence:",tokens)
print("Filterd Sentence:",filtered_text)


from nltk.stem import PorterStemmer
e_words= ["wait", "waiting", "waited", "waits"]
ps =PorterStemmer()
for w in e_words:
    rootWord=ps.stem(w)
print(rootWord)


from nltk.stem import WordNetLemmatizer
wordnet_lemmatizer = WordNetLemmatizer()
text = "studies studying cries cry"
tokenization = nltk.word_tokenize(text)
for w in tokenization:
    print("Lemma for {} is {}".format(w,wordnet_lemmatizer.lemmatize(w)))

import nltk
from nltk.tokenize import word_tokenize
data="The pink sweater fit her perfectly"
words=word_tokenize(data)
for word in words:
    print(nltk.pos_tag([word]))

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer

documentA = 'Jupiter is the largest Planet'
documentB = 'Mars is the fourth planet from the Sun'

bagOfWordsA = documentA.split(' ')
bagOfWordsB = documentB.split(' ')

uniqueWords = set(bagOfWordsA).union(set(bagOfWordsB))

numOfWordsA = dict.fromkeys(uniqueWords, 0)
for word in bagOfWordsA:
    numOfWordsA[word] += 1
    numOfWordsB = dict.fromkeys(uniqueWords, 0)
for word in bagOfWordsB:
    numOfWordsB[word] += 1

def computeTF(wordDict, bagOfWords):
    tfDict = {}
    bagOfWordsCount = len(bagOfWords)
    for word, count in wordDict.items():
        tfDict[word] = count / float(bagOfWordsCount)
    return tfDict

tfA = computeTF(numOfWordsA, bagOfWordsA)
tfB = computeTF(numOfWordsB, bagOfWordsB

def computeTFIDF(tfBagOfWords, idfs):
    tfidf = {}
    for word, val in tfBagOfWords.items():
        tfidf[word] = val * idfs[word]
    return tfidf

tfidfA = computeTFIDF(tfA, idfs)
tfidfB = computeTFIDF(tfB, idfs)
df = pd.DataFrame([tfidfA, tfidfB])
df

